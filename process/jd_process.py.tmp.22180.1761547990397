import re
import json
from typing import List, Dict
import sys
import os
from dotenv import load_dotenv

parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

# Load environment variables from .env file
load_dotenv(os.path.join(parent_dir, '.env'))

from prompt.extract_job_description import generate_job_description_prompt
import google.generativeai as genai

class JDPreprocessor:
    def __init__(self, api_key=None):
        if api_key is None:
            api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("Google API key not found. Please set GOOGLE_API_KEY in .env file")
        genai.configure(api_key=api_key)
        self.llm_client = genai.GenerativeModel("gemini-2.5-flash")
        
    def normalize_text(self, text: str) -> str:
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'http[s]?://\S+', '[URL]', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def parse_with_llm(self, jd_text: str) -> Dict[str, str]:
        prompt = generate_job_description_prompt(jd_text)

        response = self.llm_client.generate_content(prompt)
        response_text = response.text.strip()
        try:
       
            data = json.loads(response_text)
        except json.JSONDecodeError:
    
            match = re.search(r'\{.*\}', response_text, re.DOTALL)
            data = json.loads(match.group()) if match else {}
        
        for key, val in data.items():
            data[key] = self.normalize_text(val or "")
        data["full_text"] = self.normalize_text(jd_text)
        return data
    
    def generate_hybrid_chunks(
            self, 
            jd_json: Dict[str, str], 
            jd_id: str = None
        ) -> List[Dict[str, str]]:

            if jd_id is None:
                jd_id = f"jd_{uuid.uuid4().hex[:12]}"
            
            chunks = []
            
            def add_chunk(field_name: str, content: str):
                """
                添加 chunk 到列表
                """
                if content.strip():
                    chunks.append({
                        "chunk_id": f"{jd_id}_{field_name}",
                        "field": field_name,
                        "content": content.strip(),
                        "metadata": {
                            "document_id": jd_id,  # ← 关键：必须有这个
                            "document_type": "job_description",  #
                            "field": field_name,
                        }
                    })
            
            # Chunk 1: Requirements + Responsibilities
            # 职位要求和职责（最重要，用于匹配技能和经验）
            req_resp = f"""
    Requirements:
    {jd_json.get('requirements', '')}

    Responsibilities:
    {jd_json.get('responsibilities', '')}
            """.strip()
            add_chunk("requirements_responsibilities", req_resp)
            
            # Chunk 2: Experience + Education
            # 经验和学历要求
            exp_edu = f"""
    Experience Required:
    {jd_json.get('experience', '')}

    Education Required:
    {jd_json.get('education', '')}
            """.strip()
            add_chunk("experience_education", exp_edu)
            
            # Chunk 3: Benefits + Location + Salary
            # 福利、地点、薪资（可选信息）
            ben_loc_sal = f"""
    Benefits:
    {jd_json.get('benefits', '')}

    Location:
    {jd_json.get('location', '')}

    Salary:
    {jd_json.get('salary', '')}
            """.strip()
            add_chunk("benefits_location_salary", ben_loc_sal)
            
            return chunks
        
    def preprocess_jd(self, jd_text: str, jd_id: str) -> List[Dict[str, str]]:
        try:
            sections = self.parse_with_llm(jd_text) 
            return self.generate_hybrid_chunks(sections, jd_id)
        except Exception as e:
            print(f"[Warning] LLM parsing failed: {e}.")
        
