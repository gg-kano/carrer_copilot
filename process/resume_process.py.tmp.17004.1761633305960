import sys
import os
from dotenv import load_dotenv

parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

# Load environment variables from .env file
load_dotenv(os.path.join(parent_dir, '.env'))

from prompt.extract_resume import generate_resume_extraction_prompt
from typing import List, Dict
import re
import json
import google.generativeai as genai

class ResumePreprocessor:
    def __init__(self, api_key=None):
        if api_key is None:
            api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("Google API key not found. Please set GOOGLE_API_KEY in .env file")
        genai.configure(api_key=api_key)
        self.llm_client = genai.GenerativeModel("gemini-2.5-flash")
        #self.llm_client = genai.Client(api_key=api_key)
        
    def normalize_text(self, text: str) -> str:
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'http[s]?://\S+', '[URL]', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def parse_with_llm(self, resume_text: str) -> Dict:
        prompt = generate_resume_extraction_prompt(resume_text)

        response = self.llm_client.generate_content(prompt)
        response_text = response.text.strip()
        try:
            # Remove markdown code blocks if present
            if response_text.startswith("```"):
                response_text = re.sub(r'^```(?:json)?\s*', '', response_text)
                response_text = re.sub(r'\s*```$', '', response_text)

            data = json.loads(response_text)
        except json.JSONDecodeError:
            match = re.search(r'\{.*\}', response_text, re.DOTALL)
            data = json.loads(match.group()) if match else {}

        # Ensure all required fields exist with default empty arrays
        data.setdefault("experience", [])
        data.setdefault("skills", [])
        data.setdefault("education", [])
        data.setdefault("projects", [])

        return data

    def generate_resume_chunks(self, resume_json, resume_id):

        chunks = []

        # Skills
        if resume_json.get("skills"):
            chunks.append({
                "chunk_id": f"{resume_id}_skills",
                "field": "skills",
                "content": "Skills: " + ", ".join(resume_json["skills"]),
                "metadata": {
                    "document_id": resume_id,
                    "document_type": "resume",
                    "field": "skills"
                }
            })
            
        for i, exp in enumerate(resume_json.get("experience", [])):
            desc = " ".join(exp.get("description", []))
            text = f"Company: {exp.get('company', '')} | Title: {exp.get('title', '')} | {desc}"
            chunks.append({
                "chunk_id": f"{resume_id}_{i}_experience",
                "field": "experience",
                "content": text.strip(),
                "metadata": {
                    "document_id": resume_id,
                    "document_type": "resume",
                    "field": "experience"
                }
            })

        for i, edu in enumerate(resume_json.get("education", [])):
            degree = edu.get("degree", "")
            school = edu.get("school", "")
            text = f"Education: {degree} at {school}."
            chunks.append({
                "chunk_id": f"{resume_id}_{i}_education",
                "field": "education",
                "content": text.strip(),
                "metadata": {
                    "document_id": resume_id,
                    "document_type": "resume",
                    "field": "education"
                }
            })
            
        for i, proj in enumerate(resume_json.get("projects", [])):
            name = proj.get("name", "")
            desc = proj.get("description", "")
            text = f"Project: {name} â€” {desc}"
            chunks.append({
                "chunk_id": f"{resume_id}_{i}_projects",
                "field": "projects",
                "content": text.strip(),
                "metadata": {
                    "document_id": resume_id,
                    "document_type": "resume",
                    "field": "projects"
                }
            })
        
        return chunks
        
    def preprocess_resume(self, resume_text: str, resume_id: str = None) -> List[Dict[str, str]]:
        #try:
        sections = self.parse_with_llm(resume_text) 
        print(f"Extracted sections: {sections}")
        print(f"Using resume ID: {resume_id}")
        print(sections)
        return self.generate_resume_chunks(sections, resume_id)
        # except Exception as e:
        #     print(f"Error during resume preprocessing: {e}")
        #     return []
