import numpy as np
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
class AdvancedMatcher:

    def __init__(self, storage):
        self.storage = storage
        
        # å­—æ®µæƒé‡
        self.field_weights = {
            "skills_experience": 0.5,
            "education_certifications": 0.3,
            "projects_achievements": 0.2
        }
        
        # è¯„åˆ†ç­–ç•¥
        self.scoring_strategies = {
            "average": self._average_score,
            "max": self._max_score,
            "weighted_average": self._weighted_average_score
        }
    
    def match_resumes_to_jd(
        self,
        jd_id: str,
        top_k: int = 10,
        min_score: float = 0.0,
        required_fields: Optional[List[str]] = None,
        strategy: str = "weighted_average"
    ) -> List[Dict]:
        """
        é«˜çº§åŒ¹é…
        
        Args:
            jd_id: JD ID
            top_k: è¿”å›æ•°é‡
            min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
            required_fields: å¿…é¡»åŒ¹é…çš„å­—æ®µ
            strategy: è¯„åˆ†ç­–ç•¥ ('average', 'max', 'weighted_average')
        """
        
        # è·å– JD chunks
        jd_chunks = self.storage.get_chunks_by_document(jd_id)
        
        if not jd_chunks:
            return []
        
        print(f"ğŸ“‹ JD åŒ…å« {len(jd_chunks)} ä¸ª chunks")
        
        # æ”¶é›†æ‰€æœ‰åŒ¹é…
        resume_matches = defaultdict(lambda: {
            "field_scores": defaultdict(list),
            "all_chunks": []
        })
        
        # å¯¹æ¯ä¸ª JD chunk è¿›è¡Œæœç´¢
        for jd_chunk in jd_chunks:
            field = jd_chunk['metadata']['field']
            
            similar_chunks = self.storage.search_similar_chunks(
                query_text=jd_chunk['content'],
                document_type="resume",
                field=field,
                top_k=20
            )
            
            for match in similar_chunks:
                resume_id = match['metadata']['document_id']
                resume_matches[resume_id]["field_scores"][field].append(
                    match['similarity']
                )
                resume_matches[resume_id]["all_chunks"].append({
                    "jd_field": field,
                    "resume_chunk": match['chunk_id'],
                    "similarity": match['similarity'],
                    "content": match['content'][:100]
                })
        
        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        scoring_func = self.scoring_strategies.get(
            strategy, 
            self._weighted_average_score
        )
        
        final_scores = []
        
        for resume_id, data in resume_matches.items():
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if required_fields:
                has_all_required = all(
                    field in data["field_scores"] 
                    for field in required_fields
                )
                if not has_all_required:
                    continue
            
            # è®¡ç®—åˆ†æ•°
            score, field_details = scoring_func(data["field_scores"])
            
            # è¿‡æ»¤ä½åˆ†
            if score < min_score:
                continue
            
            final_scores.append({
                "resume_id": resume_id,
                "score": score,
                "field_scores": field_details,
                "matched_fields": list(data["field_scores"].keys()),
                "total_matches": len(data["all_chunks"]),
                "top_matches": sorted(
                    data["all_chunks"], 
                    key=lambda x: x['similarity'], 
                    reverse=True
                )[:3]  # å‰3ä¸ªæœ€åŒ¹é…çš„ chunks
            })
        
        # æ’åº
        final_scores.sort(key=lambda x: x['score'], reverse=True)
        
        return final_scores[:top_k]
    
    def _average_score(self, field_scores: Dict) -> Tuple[float, Dict]:
        """å¹³å‡åˆ†ç­–ç•¥"""
        all_scores = []
        field_details = {}
        
        for field, scores in field_scores.items():
            avg = sum(scores) / len(scores)
            all_scores.append(avg)
            field_details[field] = {"score": avg, "count": len(scores)}
        
        final_score = sum(all_scores) / len(all_scores) if all_scores else 0
        return final_score, field_details
    
    def _max_score(self, field_scores: Dict) -> Tuple[float, Dict]:
        """æœ€å¤§åˆ†ç­–ç•¥ï¼ˆå–æœ€å¥½çš„åŒ¹é…ï¼‰"""
        field_details = {}
        max_scores = []
        
        for field, scores in field_scores.items():
            max_score = max(scores)
            max_scores.append(max_score)
            field_details[field] = {"score": max_score, "count": len(scores)}
        
        final_score = max(max_scores) if max_scores else 0
        return final_score, field_details
    
    def _weighted_average_score(self, field_scores: Dict) -> Tuple[float, Dict]:
        """åŠ æƒå¹³å‡ç­–ç•¥"""
        weighted_score = 0.0
        total_weight = 0.0
        field_details = {}
        
        for field, scores in field_scores.items():
            avg_score = sum(scores) / len(scores)
            weight = self.field_weights.get(field, 0.1)
            
            weighted_score += avg_score * weight
            total_weight += weight
            
            field_details[field] = {
                "score": avg_score,
                "weight": weight,
                "weighted_score": avg_score * weight,
                "count": len(scores)
            }
        
        final_score = weighted_score / total_weight if total_weight > 0 else 0
        return final_score, field_details
    
    def get_match_explanation(
        self, 
        jd_id: str, 
        resume_id: str
    ) -> Dict:
        """
        è¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆè¿™ä»½ç®€å†åŒ¹é…è¿™ä¸ª JD
        """
        jd_chunks = self.storage.get_chunks_by_document(jd_id)
        resume_chunks = self.storage.get_chunks_by_document(resume_id)
        
        explanations = []
        
        for jd_chunk in jd_chunks:
            # æ‰¾åˆ°æœ€åŒ¹é…çš„ resume chunk
            similar = self.storage.search_similar_chunks(
                query_text=jd_chunk['content'],
                document_type="resume",
                top_k=1
            )
            
            if similar and similar[0]['metadata']['document_id'] == resume_id:
                explanations.append({
                    "jd_requirement": jd_chunk['content'][:200],
                    "resume_match": similar[0]['content'][:200],
                    "similarity": similar[0]['similarity'],
                    "field": jd_chunk['metadata']['field']
                })
        
        return {
            "jd_id": jd_id,
            "resume_id": resume_id,
            "matches": explanations,
            "overall_match_count": len(explanations)
        }


# ===== å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    from chroma_storage import ChromaDBStorage
    
    # åˆå§‹åŒ–
    storage = ChromaDBStorage()
    matcher = AdvancedMatcher(storage)
    
    # åŒ¹é…
    print("ğŸš€ å¼€å§‹åŒ¹é…...")
    results = matcher.match_resumes_to_jd(
        jd_id="jd_001",
        top_k=10,
        min_score=0.5,  # åªè¿”å›åˆ†æ•° > 0.5 çš„
        required_fields=["skills_experience"],  # å¿…é¡»æœ‰æŠ€èƒ½ç»éªŒåŒ¹é…
        strategy="weighted_average"
    )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*70)
    print("ğŸ¯ åŒ¹é…ç»“æœ")
    print("="*70)
    
    for i, result in enumerate(results, 1):
        print(f"\n{'='*70}")
        print(f"æ’å #{i}: {result['resume_id']}")
        print(f"æ€»åˆ†: {result['score']:.2%}")
        print(f"åŒ¹é…å­—æ®µ: {', '.join(result['matched_fields'])}")
        print(f"æ€»åŒ¹é…æ•°: {result['total_matches']}")
        
        print("\nå„å­—æ®µè¯¦æƒ…:")
        for field, details in result['field_scores'].items():
            print(f"  {field}:")
            print(f"    åˆ†æ•°: {details['score']:.2%}")
            if 'weight' in details:
                print(f"    æƒé‡: {details['weight']}")
                print(f"    åŠ æƒåˆ†: {details['weighted_score']:.2%}")
        
        print("\nå‰3ä¸ªæœ€ä½³åŒ¹é…:")
        for j, match in enumerate(result['top_matches'], 1):
            print(f"  {j}. [{match['jd_field']}] ç›¸ä¼¼åº¦ {match['similarity']:.2%}")
            print(f"     {match['content']}...")
    
    # è¯¦ç»†è§£é‡ŠæŸä¸ªåŒ¹é…
    if results:
        print("\n" + "="*70)
        print("ğŸ“– è¯¦ç»†åŒ¹é…è§£é‡Š")
        print("="*70)
        
        explanation = matcher.get_match_explanation(
            jd_id="jd_001",
            resume_id=results[0]['resume_id']
        )
        
        print(f"\nJD: {explanation['jd_id']}")
        print(f"Resume: {explanation['resume_id']}")
        print(f"åŒ¹é…ç‚¹æ•°: {explanation['overall_match_count']}\n")
        
        for i, match in enumerate(explanation['matches'], 1):
            print(f"{i}. [{match['field']}] ç›¸ä¼¼åº¦: {match['similarity']:.2%}")
            print(f"   JDè¦æ±‚: {match['jd_requirement']}...")
            print(f"   ç®€å†åŒ¹é…: {match['resume_match']}...")
            print()

